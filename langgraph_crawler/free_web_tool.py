"""å…è´¹çš„ç½‘é¡µæŠ“å–å·¥å…· - æ›¿ä»£Jina API"""
import requests
import os
import hashlib
from typing import Optional
from urllib.parse import urlparse, urljoin
from langchain_core.tools import tool
from bs4 import BeautifulSoup
import time
import json

class FreeWebScrapingHelper:
    """å…è´¹ç½‘é¡µæŠ“å–è¾…åŠ©ç±»"""
    
    def __init__(self, max_content_length: int = 2000, work_dir: str = None):
        self.max_content_length = max_content_length
        self.work_dir = work_dir or "output"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
    
    def _trim_content(self, content: str) -> str:
        """è£å‰ªå†…å®¹åˆ°æœ€å¤§å…è®¸é•¿åº¦"""
        if len(content) > self.max_content_length:
            truncated = content[:self.max_content_length]
            return truncated + "... (content truncated)"
        return content
    
    @staticmethod
    def _generate_file_name_from_url(url: str, max_length=255) -> str:
        """ä»URLç”Ÿæˆæ–‡ä»¶å"""
        url_bytes = url.encode("utf-8")
        hash_value = hashlib.blake2b(url_bytes).hexdigest()
        parsed_url = urlparse(url)
        domain = parsed_url.netloc.replace('www.', '')
        file_name = f"{domain}_{hash_value[:8]}.md"
        return file_name
    
    def _html_to_markdown(self, html_content: str, url: str) -> str:
        """å°†HTMLè½¬æ¢ä¸ºMarkdownæ ¼å¼"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ç§»é™¤è„šæœ¬å’Œæ ·å¼
        for script in soup(["script", "style", "nav", "footer", "header", "aside"]):
            script.decompose()
        
        # æå–æ ‡é¢˜
        title = soup.find('title')
        title_text = title.get_text().strip() if title else "æ— æ ‡é¢˜"
        
        # æå–ä¸»è¦å†…å®¹
        content_selectors = [
            'article', 'main', '.content', '.post-content', 
            '.article-content', '.entry-content', '#content'
        ]
        
        main_content = None
        for selector in content_selectors:
            main_content = soup.select_one(selector)
            if main_content:
                break
        
        if not main_content:
            main_content = soup.find('body') or soup
        
        # è½¬æ¢ä¸ºMarkdown
        markdown_content = f"# {title_text}\n\n"
        markdown_content += f"**URLæ¥æº**: {url}\n\n"
        
        # æå–æ®µè½å’Œæ ‡é¢˜
        for element in main_content.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'li', 'div']):
            text = element.get_text().strip()
            if not text:
                continue
                
            if element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                level = int(element.name[1])
                markdown_content += f"\n{'#' * level} {text}\n\n"
            elif element.name == 'li':
                markdown_content += f"- {text}\n"
            elif len(text) > 10:  # åªä¿ç•™æœ‰æ„ä¹‰çš„æ–‡æœ¬
                markdown_content += f"{text}\n\n"
        
        return markdown_content

# åˆ›å»ºå…¨å±€è¾…åŠ©å®ä¾‹
_free_scraper = FreeWebScrapingHelper()

@tool
def free_url_reader(url: str) -> str:
    """è¯»å–ç½‘é¡µå†…å®¹å¹¶è½¬æ¢ä¸ºMarkdownæ ¼å¼
    
    è¿™ä¸ªå·¥å…·ç”¨äºé«˜è´¨é‡çš„ç½‘é¡µå†…å®¹æå–ï¼š
    - ä½¿ç”¨requestså’ŒBeautifulSoupæŠ“å–ç½‘é¡µ
    - è‡ªåŠ¨æ¸…ç†HTMLæ ‡ç­¾ï¼Œæå–æ ¸å¿ƒæ–‡æœ¬å†…å®¹
    - è½¬æ¢ä¸ºæ˜“è¯»çš„Markdownæ ¼å¼å¹¶è‡ªåŠ¨ä¿å­˜
    - å¤„ç†å„ç§ç½‘é¡µç±»å‹ï¼šæ–°é—»ã€åšå®¢ã€æ–‡æ¡£ç­‰
    
    Args:
        url: è¦è¯»å–çš„ç½‘é¡µURL
            - æ”¯æŒHTTPå’ŒHTTPSåè®®
            - å¯ä»¥æ˜¯æ–°é—»ç½‘ç«™ã€åšå®¢ã€å®˜æ–¹æ–‡æ¡£ç­‰
            - ä¾‹å¦‚ï¼š"https://example.com/article"
            - å·¥å…·ä¼šè‡ªåŠ¨å¤„ç†ç¼–ç å’Œæ ¼å¼é—®é¢˜
        
    Returns:
        Markdownæ ¼å¼çš„ç½‘é¡µå†…å®¹ï¼ŒåŒ…æ‹¬ï¼š
        - æ¸…æ´çš„æ–‡æœ¬å†…å®¹ï¼Œå»é™¤å¹¿å‘Šå’Œæ— å…³å…ƒç´ 
        - ä¿ç•™æ ‡é¢˜ã€æ®µè½ã€åˆ—è¡¨ç­‰ç»“æ„
        - è‡ªåŠ¨ä¿å­˜åˆ°æ–‡ä»¶ï¼Œè¿”å›å†…å®¹æ‘˜è¦
        - å¦‚æœæŠ“å–å¤±è´¥ï¼Œè¿”å›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        
    ä½¿ç”¨åœºæ™¯ï¼š
    - æŠ“å–æ–°é—»æ–‡ç« è¿›è¡Œåˆ†æ
    - è·å–äº§å“ä¿¡æ¯å’Œè¯„ä»·
    - æ”¶é›†ç ”ç©¶èµ„æ–™
    - ç›‘æ§ç½‘ç«™å†…å®¹å˜åŒ–
    
    """
    try:
        print(f"ğŸŒ æ­£åœ¨æŠ“å–: {url}")
        
        # å‘é€è¯·æ±‚
        response = requests.get(url, headers=_free_scraper.headers, timeout=30)
        response.raise_for_status()
        response.encoding = response.apparent_encoding  # è‡ªåŠ¨æ£€æµ‹ç¼–ç 
        
        # è½¬æ¢ä¸ºMarkdown
        markdown_content = _free_scraper._html_to_markdown(response.text, url)
        
        # ä¿å­˜å®Œæ•´å†…å®¹åˆ°æ–‡ä»¶
        # if markdown_content:
        #     filename = _free_scraper._generate_file_name_from_url(url)
        #     save_path = os.path.join(_free_scraper.work_dir, filename)
        #     os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
        #     with open(save_path, 'w', encoding='utf-8') as f:
        #         f.write(markdown_content)
        #     print(f"ğŸ’¾ å†…å®¹å·²ä¿å­˜åˆ°: {filename}")
        
        # è¿”å›æˆªæ–­çš„å†…å®¹é¢„è§ˆ
        result = _free_scraper._trim_content(markdown_content)
        return result
        
    except requests.exceptions.RequestException as e:
        return f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {str(e)}"
    except Exception as e:
        return f"æŠ“å–ç½‘é¡µæ—¶å‡ºé”™: {str(e)}"

@tool
def free_search(query: str, num_results: int = 5) -> str:
    """ä½¿ç”¨æœç´¢APIæ‰§è¡Œç½‘é¡µæœç´¢

    è¿™ä¸ªå·¥å…·æä¾›ç½‘é¡µæœç´¢åŠŸèƒ½ï¼š
    - ä½¿ç”¨DuckDuckGoæœç´¢å¼•æ“
    - è¿”å›ç›¸å…³æ€§å¼ºçš„æœç´¢ç»“æœ
    - è‡ªåŠ¨è¿‡æ»¤å’Œæ•´ç†æœç´¢ç»“æœ
    - æ”¯æŒä¸­è‹±æ–‡æœç´¢
    
    Args:
        query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
              - å¯ä»¥æ˜¯å…³é”®è¯ï¼šä¾‹å¦‚"36kr åˆ›æŠ• èèµ„"
              - å¯ä»¥æ˜¯é—®é¢˜ï¼šä¾‹å¦‚"36æ°ªæœ€æ–°èèµ„æ–°é—»"
              - å¯ä»¥æ˜¯å…·ä½“éœ€æ±‚ï¼šä¾‹å¦‚"36kråˆ›æŠ•å¹³å°å®˜ç½‘"
              - æ”¯æŒå¤æ‚æŸ¥è¯¢å’Œä¸­æ–‡æœç´¢
        num_results: è¿”å›ç»“æœæ•°é‡ï¼ˆé»˜è®¤5ä¸ªï¼‰
        
    Returns:
        JSONæ ¼å¼çš„æœç´¢ç»“æœï¼ŒåŒ…æ‹¬ï¼š
        - å¤šä¸ªç›¸å…³ç½‘é¡µçš„æ ‡é¢˜ã€æ‘˜è¦å’Œé“¾æ¥
        - æŒ‰ç›¸å…³æ€§æ’åºçš„ç»“æœåˆ—è¡¨
        - è‡ªåŠ¨ä¿å­˜æœç´¢ç»“æœåˆ°æ–‡ä»¶
        - å¦‚æœæœç´¢å¤±è´¥ï¼Œè¿”å›é”™è¯¯è¯¦æƒ…
        
    ä½¿ç”¨åœºæ™¯ï¼š
    - æœç´¢ç‰¹å®šç½‘ç«™çš„é¡µé¢
    - æŸ¥æ‰¾æ–°é—»å’Œèµ„è®¯
    - æ”¶é›†è¡Œä¸šä¿¡æ¯
    - å¯»æ‰¾å®˜æ–¹ç½‘ç«™å’Œèµ„æº
    
    """
    try:
        from duckduckgo_search import DDGS
        
        print(f"ğŸ” æ­£åœ¨æœç´¢: {query}")
        
        # ä½¿ç”¨DuckDuckGoæœç´¢
        with DDGS() as ddgs:
            results = list(ddgs.text(query, max_results=num_results))
        
        if not results:
            return "æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœ"
        
        # æ ¼å¼åŒ–æœç´¢ç»“æœ
        formatted_results = {
            "query": query,
            "results": []
        }
        
        for i, result in enumerate(results, 1):
            formatted_result = {
                "rank": i,
                "title": result.get('title', 'æ— æ ‡é¢˜'),
                "url": result.get('href', ''),
                "description": result.get('body', 'æ— æè¿°'),
            }
            formatted_results["results"].append(formatted_result)
        
        # ä¿å­˜æœç´¢ç»“æœ
        # search_filename = f"search_{hashlib.md5(query.encode()).hexdigest()[:8]}.json"
        # search_path = os.path.join(_free_scraper.work_dir, search_filename)
        # os.makedirs(os.path.dirname(search_path), exist_ok=True)
        
        # with open(search_path, 'w', encoding='utf-8') as f:
        #     json.dump(formatted_results, f, ensure_ascii=False, indent=2)
        
        # print(f"ğŸ’¾ æœç´¢ç»“æœå·²ä¿å­˜åˆ°: {search_filename}")
        
        # è¿”å›æ ¼å¼åŒ–çš„ç»“æœ
        result_text = f"æœç´¢æŸ¥è¯¢: {query}\n\n"
        for result in formatted_results["results"]:
            result_text += f"{result['rank']}. **{result['title']}**\n"
            result_text += f"   URL: {result['url']}\n"
            result_text += f"   æè¿°: {result['description']}\n\n"
        
        return _free_scraper._trim_content(result_text)
        
    except ImportError:
        return "é”™è¯¯: éœ€è¦å®‰è£… duckduckgo-search åº“ã€‚è¯·è¿è¡Œ: pip install duckduckgo-search"
    except Exception as e:
        return f"æœç´¢æ—¶å‡ºé”™: {str(e)}"
