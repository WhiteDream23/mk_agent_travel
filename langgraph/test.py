from typing import Annotated
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

# ä¿®æ”¹APIå¯†é’¥é…ç½®ï¼Œæ”¯æŒå¤šç§API
openai_api_key = os.getenv("OPENAI_API_KEY")
gemini_api_key = os.getenv("GEMINI_API_KEY")

if not gemini_api_key and not openai_api_key:
    raise ValueError("è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® GEMINI_API_KEY æˆ– OPENAI_API_KEY")

# æ ¹æ®å¯ç”¨çš„APIå¯†é’¥é€‰æ‹©æ¨¡å‹
if gemini_api_key:
    
    llm = ChatOpenAI(
        model="gemini-2.5-flash",
       base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
        api_key=gemini_api_key,
        temperature=0.7,
        max_tokens=2000
    )
    print("âœ… ä½¿ç”¨ Google Gemini API")
    
elif openai_api_key:

    
    llm = ChatOpenAI(
        model="Qwen/Qwen3-8B",
        base_url="https://api.siliconflow.cn/v1",
        api_key=openai_api_key,
        temperature=0.7,
        max_tokens=2000
    )
    print("âœ… ä½¿ç”¨ç¡…åŸºæµåŠ¨ API")



from langchain_openai import ChatOpenAI
from typing import Annotated

# å°è¯•å¯¼å…¥TavilySearchï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
try:
    from langchain_tavily import TavilySearch
    TAVILY_AVAILABLE = True
except ImportError:
    print("âš ï¸  TavilySearchæœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ— å·¥å…·æ¨¡å¼")
    print("ğŸ’¡ å¦‚éœ€æœç´¢åŠŸèƒ½ï¼Œè¯·è¿è¡Œ: pip install langchain-tavily-search")
    TAVILY_AVAILABLE = False

from langchain_core.messages import BaseMessage
from typing_extensions import TypedDict
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.types import Command, interrupt
from langchain_core.tools import tool,InjectedToolCallId
# æ¡ä»¶å¯¼å…¥LangGraphé¢„æ„å»ºå·¥å…·
if TAVILY_AVAILABLE:
    from langgraph.prebuilt import ToolNode, tools_condition



class State(TypedDict):
    messages: Annotated[list, add_messages]
    name: str
    birthday: str

graph_builder = StateGraph(State)

# æ ¹æ®æ˜¯å¦æœ‰Tavilyæ¥é…ç½®å·¥å…·
if TAVILY_AVAILABLE:
    try:
        # tool = TavilySearch(max_results=2)
        # tools = [tool]
        @tool
        def human_assistance(
            name: str, birthday: str, tool_call_id: Annotated[str, InjectedToolCallId]
        ) -> str:
            """Request assistance from a human."""
            human_response = interrupt(
                {
                    "question": "Is this correct?",
                    "name": name,
                    "birthday": birthday,
                },
            )
            # If the information is correct, update the state as-is.
            if human_response.get("correct", "").lower().startswith("y"):
                verified_name = name
                verified_birthday = birthday
                response = "Correct"
            # Otherwise, receive information from the human reviewer.
            else:
                verified_name = human_response.get("name", name)
                verified_birthday = human_response.get("birthday", birthday)
                response = f"Made a correction: {human_response}"

            # This time we explicitly update the state with a ToolMessage inside
            # the tool.
            state_update = {
                "name": verified_name,
                "birthday": verified_birthday,
                "messages": [ToolMessage(response, tool_call_id=tool_call_id)],
            }
            # We return a Command object in the tool to update our state.
            return Command(update=state_update)

        tool = TavilySearch(max_results=2)
        tools = [tool, human_assistance]
        llm_with_tools = llm.bind_tools(tools)
        print("âœ… Tavilyæœç´¢å·¥å…·å·²é…ç½®")
    except Exception as e:
        print(f"âš ï¸  Tavilyé…ç½®å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·æ£€æŸ¥TAVILY_API_KEYç¯å¢ƒå˜é‡")
        tools = []
        llm_with_tools = llm
else:
    tools = []
    llm_with_tools = llm

# def chatbot(state: State):
#     return {"messages": [llm_with_tools.invoke(state["messages"])]}
def chatbot(state: State):
    message = llm_with_tools.invoke(state["messages"])
    # Because we will be interrupting during tool execution,
    # we disable parallel tool calling to avoid repeating any
    # tool invocations when we resume.
    assert len(message.tool_calls) <= 1
    return {"messages": [message]}

graph_builder.add_node("chatbot", chatbot)

# åªæœ‰åœ¨æœ‰å·¥å…·æ—¶æ‰æ·»åŠ å·¥å…·èŠ‚ç‚¹å’Œç›¸å…³è¾¹
if tools and TAVILY_AVAILABLE:
    tool_node = ToolNode(tools=tools)
    graph_builder.add_node("tools", tool_node)
    
    graph_builder.add_conditional_edges(
        "chatbot",
        tools_condition,
    )
    # Any time a tool is called, we return to the chatbot to decide the next step
    graph_builder.add_edge("tools", "chatbot")

graph_builder.add_edge(START, "chatbot")
# graph = graph_builder.compile()
memory = InMemorySaver()
graph = graph_builder.compile(checkpointer=memory)
# ç”Ÿæˆå¹¶ä¿å­˜å›¾å½¢ç»“æ„
def save_graph_visualization():
    """ç”Ÿæˆå¹¶ä¿å­˜LangGraphçš„å¯è§†åŒ–å›¾å½¢"""
    try:
        # å°è¯•ç”ŸæˆMermaidå›¾å¹¶ä¿å­˜ä¸ºPNG
        png_data = graph.get_graph().draw_mermaid_png()
        with open("langgraph_structure.png", "wb") as f:
            f.write(png_data)
        print("âœ… å›¾å½¢ç»“æ„å·²ä¿å­˜ä¸º 'langgraph_structure.png'")
        
        # å¦‚æœåœ¨Jupyterç¯å¢ƒä¸­ï¼Œå°è¯•æ˜¾ç¤ºå›¾ç‰‡
        try:
            from IPython.display import Image, display
            display(Image(png_data))
            print("âœ… å›¾å½¢ç»“æ„å·²åœ¨notebookä¸­æ˜¾ç¤º")
        except:
            print("ğŸ’¡ åœ¨æ™®é€šPythonç¯å¢ƒä¸­è¿è¡Œï¼Œå›¾ç‰‡å·²ä¿å­˜åˆ°æ–‡ä»¶")
            
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–åŒ…: {e}")
        print("ğŸ’¡ è¯·å®‰è£…: pip install pygraphviz æˆ– pip install graphviz")
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå›¾å½¢æ—¶å‡ºé”™: {e}")
        print("ğŸ’¡ å¦‚æœæ‚¨æƒ³æŸ¥çœ‹å›¾å½¢ç»“æ„ï¼Œè¯·ç¡®ä¿å®‰è£…äº†graphvizç›¸å…³ä¾èµ–")

# å°è¯•ç”Ÿæˆå›¾å½¢
print("ğŸ”„ æ­£åœ¨ç”ŸæˆLangGraphç»“æ„å›¾...")
save_graph_visualization()



config = {"configurable": {"thread_id": "1"}}

def stream_graph_updates(user_input: str):
    """å¤„ç†ç”¨æˆ·è¾“å…¥å¹¶æ”¯æŒäººæœºåä½œ"""
    try:
        # å‘é€ç”¨æˆ·æ¶ˆæ¯åˆ°å›¾å½¢
        events = graph.stream(
            {"messages": [{"role": "user", "content": user_input}]},
            config,
            stream_mode="values",
        )
        
        for event in events:
            if "messages" in event and event["messages"]:
                event["messages"][-1].pretty_print()
                
    except Exception as e:
        print(f"âŒ å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {e}")

def handle_human_interruption():
    """å¤„ç†äººæœºåä½œä¸­æ–­"""
    try:
        # è·å–å½“å‰çŠ¶æ€ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ä¸­æ–­
        snapshot = graph.get_state(config)
        
        if snapshot.next:
            print("\nğŸ¤ AIæ­£åœ¨è¯·æ±‚äººå·¥ååŠ©...")
            
            # æ˜¾ç¤ºä¸­æ–­çš„åŸå› 
            print("ğŸ’­ AIéœ€è¦æ‚¨çš„å¸®åŠ©æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜")
            
            # è·å–äººå·¥è¾“å…¥
            human_input = input("\nğŸ‘¤ è¯·æä¾›å¸®åŠ© (æˆ–è¾“å…¥ 'skip' è·³è¿‡): ").strip()
            
            if human_input.lower() == 'skip':
                print("â­ï¸ è·³è¿‡äººå·¥ååŠ©")
                # å‘é€ç©ºå“åº”ç»§ç»­
                resume_command = Command(resume={"data": "ç”¨æˆ·é€‰æ‹©è·³è¿‡äººå·¥ååŠ©"})
            else:
                # å‘é€äººå·¥å“åº”
                resume_command = Command(resume={"data": human_input})
            
            print("ğŸ”„ ç»§ç»­å¤„ç†...")
            events = graph.stream(resume_command, config, stream_mode="values")
            
            for event in events:
                if "messages" in event and event["messages"]:
                    event["messages"][-1].pretty_print()
            
            return True
            
    except Exception as e:
        print(f"âŒ å¤„ç†äººå·¥ååŠ©æ—¶å‡ºé”™: {e}")
        # å°è¯•å‘é€é»˜è®¤å“åº”æ¥æ¢å¤
        try:
            resume_command = Command(resume={"data": "æŠ±æ­‰ï¼Œäººå·¥ååŠ©æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç»§ç»­ä¸AIå¯¹è¯"})
            events = graph.stream(resume_command, config, stream_mode="values")
            for event in events:
                if "messages" in event and event["messages"]:
                    event["messages"][-1].pretty_print()
        except:
            pass
        return False
    
    return False

print("ğŸ¤– äººæœºåä½œèŠå¤©æœºå™¨äººå·²å¯åŠ¨!")
print("ğŸ’¡ å½“AIéœ€è¦å¸®åŠ©æ—¶ï¼Œæ‚¨å¯ä»¥æä¾›äººå·¥ååŠ©")
print("ğŸ“ è¾“å…¥ 'quit', 'exit' æˆ– 'q' é€€å‡ºç¨‹åº\n")

while True:
    try:
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰å¾…å¤„ç†çš„äººå·¥ååŠ©è¯·æ±‚
        try:
            snapshot = graph.get_state(config)
            if snapshot and snapshot.next:
                print("ğŸ” æ£€æµ‹åˆ°å¾…å¤„ç†çš„äººå·¥ååŠ©è¯·æ±‚...")
                if handle_human_interruption():
                    continue
        except Exception as check_error:
            # å¦‚æœæ£€æŸ¥çŠ¶æ€å¤±è´¥ï¼Œç»§ç»­æ­£å¸¸æµç¨‹
            pass
            
        user_input = input("User: ")
        if user_input.lower() in ["quit", "exit", "q"]:
            print("ğŸ‘‹ Goodbye!")
            break
            
        if user_input.strip():  # åªå¤„ç†éç©ºè¾“å…¥
            stream_graph_updates(user_input)
            
            # å¤„ç†å®Œç”¨æˆ·æ¶ˆæ¯åï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦äººå·¥ååŠ©
            try:
                while True:
                    snapshot = graph.get_state(config)
                    if snapshot and snapshot.next:
                        # æœ‰å¾…å¤„ç†çš„ä¸­æ–­ï¼Œå¤„ç†äººå·¥ååŠ©
                        if not handle_human_interruption():
                            break  # ç”¨æˆ·é€‰æ‹©è·³è¿‡æˆ–å¤„ç†å®Œæˆ
                    else:
                        break  # æ²¡æœ‰æ›´å¤šä¸­æ–­éœ€è¦å¤„ç†
            except Exception as interrupt_error:
                print(f"âš ï¸ å¤„ç†ä¸­æ–­æ—¶å‡ºé”™: {interrupt_error}")
        else:
            print("ğŸ’¡ è¯·è¾“å…¥ä¸€äº›å†…å®¹...")
            
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Goodbye!")
        break
    except EOFError:
        # fallback if input() is not available
        user_input = "What do you know about LangGraph?"
        print("User: " + user_input)
        stream_graph_updates(user_input)
        break
    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
        print("ğŸ’¡ è¯·æ£€æŸ¥APIå¯†é’¥é…ç½®å’Œç½‘ç»œè¿æ¥")
